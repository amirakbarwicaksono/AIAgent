# airquality_prediction.py
import warnings
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from ucimlrepo import fetch_ucirepo
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
from xgboost import XGBRegressor
import time
import joblib

# -------------------------
# 1) Ambil dataset dari UCI
# -------------------------
air_quality = fetch_ucirepo(id=360)
df = air_quality.data.features.copy()

# -------------------------
# 2) Siapkan fitur & target
# -------------------------
df["datetime"] = pd.to_datetime(df["Date"] + " " + df["Time"], dayfirst=True, errors="coerce")
df["hour"] = df["datetime"].dt.hour
df["dayofweek"] = df["datetime"].dt.dayofweek

y = df["CO(GT)"].copy()
X = df.drop(columns=["Date", "Time", "CO(GT)", "datetime"])

# -------------------------
# 3) Tangani missing values
# -------------------------
X = X.replace(-200, np.nan)
y = y.replace(-200, np.nan)

imputer = SimpleImputer(strategy="median")
X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns, index=X.index)

mask = y.notna()
X_clean = X_imputed.loc[mask].reset_index(drop=True)
y_clean = y.loc[mask].reset_index(drop=True)

print("Ukuran data setelah cleaning:", X_clean.shape)

# -------------------------
# 4) Train-test split
# -------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X_clean, y_clean, test_size=0.2, random_state=42
)

# -------------------------
# 5) Baseline: Random Forest
# -------------------------
rf = RandomForestRegressor(n_estimators=200, random_state=42)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)

print("\n=== Random Forest (baseline) ===")
print("MSE:", mean_squared_error(y_test, y_pred_rf))
print("R2 :", r2_score(y_test, y_pred_rf))

# -------------------------
# 6) RandomizedSearch XGBoost
# -------------------------
xgb_model = XGBRegressor(objective="reg:squarederror", random_state=42, verbosity=0)

param_dist = {
    "n_estimators": [100, 200, 300, 500],
    "max_depth": [3, 5, 7, 9, 12],
    "learning_rate": [0.01, 0.05, 0.1, 0.2],
    "subsample": [0.6, 0.8, 1.0],
    "colsample_bytree": [0.6, 0.8, 1.0],
    "min_child_weight": [1, 3, 5, 7],
    "gamma": [0, 0.1, 0.3, 0.5],
    "reg_alpha": [0, 0.01, 0.1, 1],
    "reg_lambda": [0.5, 1, 2, 5]
}

random_search = RandomizedSearchCV(
    estimator=xgb_model,
    param_distributions=param_dist,
    n_iter=50,
    scoring="r2",
    cv=3,
    verbose=2,
    n_jobs=-1,
    random_state=42
)

t0 = time.time()
random_search.fit(X_train, y_train)
t1 = time.time()

print(f"\nRandomizedSearch selesai (runtime {t1-t0:.1f}s).")
print("Best Parameters:", random_search.best_params_)
print("Best CV Score (R2):", random_search.best_score_)

best_xgb = random_search.best_estimator_
y_pred_xgb = best_xgb.predict(X_test)

print("\n=== XGBoost (RandomizedSearch best) ===")
print("MSE:", mean_squared_error(y_test, y_pred_xgb))
print("R2 :", r2_score(y_test, y_pred_xgb))

# -------------------------
# 7) Feature Importance
# -------------------------
fi = best_xgb.feature_importances_
feat_names = X_train.columns
fi_df = pd.DataFrame({"feature": feat_names, "importance": fi})
fi_df = fi_df.sort_values("importance", ascending=False).head(20)

plt.figure(figsize=(8,6))
plt.barh(fi_df["feature"][::-1], fi_df["importance"][::-1])
plt.title("Top 20 Feature Importances (XGBoost tuned)")
plt.xlabel("Importance")
plt.tight_layout()
plt.show()

# -------------------------
# 8) Simpan model
# -------------------------
joblib.dump(best_xgb, "best_xgb_airquality_co.pkl")
joblib.dump(imputer, "imputer.pkl")
print("\nModel dan imputer berhasil disimpan!")
